"""
tensorflow:1.1.6
matplotlib
numpy
"""

import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
import numpy as np
import matplotlib.pyplot as plt

tf.set_random_seed(123)
np.random.seed(123)

BATCH_SIZE = 50
LR = 0.001            # learning rate

mnist = input_data.read_data_sets('./mnist', one_hot=True)  # they has been normalized to range(0, 1)
test_x = mnist.test.images[:2000]
test_y = mnist.test.labels[:2000]

# plot one example  number 7
print(mnist.train.images.shape)   # (55000, 28*28)
print(mnist.train.labels.shape)   # (55000, 10)
plt.imshow(mnist.train.images[0].reshape((28, 28)), cmap='gray') # change sample images[*]
plt.title('%i' % np.argmax(mnist.train.labels[0])); plt.show()   # change title number [*]

tf_x = tf.placeholder(tf.float32, [None, 28*28]) / 255.    #  None 是表示所有图片的例子
image = tf.reshape(tf_x, [-1, 28, 28, 1])    # (???, height, width, channel) -1就是先不管图片所有维度，1是灰白的channel
# print(x_image.shape)  # [n_sample,28,28,1]
tf_y = tf.placeholder(tf.int32, [None, 10])  # input y

# CNN
conv1 = tf.layers.conv2d(   # shape (28, 28, 1)   row 28 column 28 image 3d to 2d,so is 1
    inputs = image,
    filters=16,
    kernel_size=5,
    strides=1,
    padding='same',
    activation=tf.nn.relu
)           # -> (28, 28, 16)
pool1 = tf.layers.max_pooling2d(
    conv1,
    pool_size=2,
    strides=2
)           # -> (14, 14, 16)
conv2 = tf.layers.conv2d(pool1, 32, 5, 1, 'same', activation=tf.nn.relu)  # -> (14, 14, 32)
pool2 = tf.layers.max_pooling2d(conv2, 2, 2)  # -> (7, 7, 32)

flat = tf.reshape(pool2, [-1, 7*7*32])   # -> (7*7*32, 32个进行扁平化处理放在一张纸上)  1d vector 全连接层  -1 先不管所在维度
output = tf.layers.dense(flat, 10)       # output layer

loss = tf.losses.softmax_cross_entropy(onehot_labels=tf_y, logits=output)  # compute cost
train_op = tf.train.AdamOptimizer(LR).minimize(loss)

accuracy = tf.metrics.accuracy(
    labels = tf.argmax(tf_y, axis=1), predictions=tf.argmax(output, axis=1),)[1]

sess = tf.Session()
# the local var is for accuracy_op
init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
sess.run(init_op)   # initialize var in graph

# following function (plot_with_labels) is for visualization, can be ignored if not interested
from matplotlib import cm
try: from sklearn.manifold import TSNE; HAS_SK = True
except: HAS_SK = False; print('\nPlease install sklearn for layer visualization\n')
def plot_with_labels(lowDWeights, labels):
    plt.cla(); X, Y= lowDWeights[:, 0], lowDWeights[:, 1]
    for x, y, s in zip(X, Y, labels):
        c = cm.rainbow(int(255 * s / 9)); plt.text(x, y, s, backgroundcolor=c, fontsize=9)
    plt.xlim(X.min(), X.max()); plt.ylim(Y.min(), Y.max())
    plt.title('Visualize last layer')
    plt.show()
    plt.pause(0.01)

plt.ion()
for step in range(1000):
    b_x, b_y = mnist.train.next_batch(BATCH_SIZE)
    _, loss_ = sess.run([train_op, loss], {tf_x:b_x, tf_y: b_y})
    if step % 50 == 0:
        accuracy_, flat_representation = sess.run([accuracy,flat], {tf_x:test_x, tf_y:test_y})
        print('Step:', step, '| train loss: %.4f' % loss_, '| test accuracy: %.2f' % accuracy_)

        if HAS_SK:
            # Visualization of trained flatten layer (T-SNE)
            tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)
            plot_only = 500
            low_dim_embs = tsne.fit_transform(flat_representation[:plot_only, :])
            labels = np.argmax(test_y, axis=1)[:plot_only]
            plot_with_labels(low_dim_embs, labels)
plt.ioff()

# print 10 predictions from test data
test_output = sess.run(output, {tf_x: test_x[:10]})
pred_y = np.argmax(test_output, 1)
print(pred_y, 'prediction number')
print(np.argmax(test_y[:10],1), 'real number')

"""
500 times
Step: 0 | train loss: 2.2555 | test accuracy: 0.11
Step: 50 | train loss: 0.4174 | test accuracy: 0.46
Step: 100 | train loss: 0.2045 | test accuracy: 0.61
Step: 150 | train loss: 0.2095 | test accuracy: 0.69
Step: 200 | train loss: 0.1779 | test accuracy: 0.74
Step: 250 | train loss: 0.1896 | test accuracy: 0.77
Step: 300 | train loss: 0.0715 | test accuracy: 0.79
Step: 350 | train loss: 0.0670 | test accuracy: 0.81
Step: 400 | train loss: 0.0632 | test accuracy: 0.83
Step: 450 | train loss: 0.0617 | test accuracy: 0.84
[7 2 1 0 4 1 4 9 5 9] prediction number
[7 2 1 0 4 1 4 9 5 9] real number
1000 tiems 0.9
"""
##
##def weight_variable(shape):  # 输入shape 返回variable的情况
##    initial = tf.truncated_normal(shape,stddev=0.1)
##    return tf.Variable(inital)
##
##def bias_variable(shape):
##    initial = tf.constant(0.1,shape=shape)
##    return tf.Variable(initial)
##
##def conv2d(x,W):
##    # stride [1, x_movement, y_movement,1]
##    return tf.nn.conv2d(x,W, strides=[1,1,1,1], padding='SAME') # Must have strides[0]
##
##def max_pool_2x2(x):
##    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')

####conv1 layer
##W_vonv1 = weight_variable([5,5,1,32]) # patch 5x5, in size 1, out size 32  1个单位 32个高度
##b_conv1 = bias_variable([32])
##h_conv1 =tf.nn.relu( conv2d(x_image,W_conv1)+b_conv1)  # output size 28x28x32
##h_pool1 = max_pool_2x2(h_conv1)  # output size 14x14x32
##
#### conv2 layer ##
##W_conv2 = weight_variable([5,5,32,64])  # patch 5x5 in size 32 out size 64
##b_conv2 = bias_variable([64])
##h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv1)+b_conv1) # output size 14x14x64
##h_pool2 = max_pool_2x2(h_conv1)
### output size 7x7x64
#### func1 layer ##
##W_fc1 = weight_variable([7*7*64,1024])  # 1024 是输出
##b_fc1 = bias_variable([1024])
### 2d -> 1d  扁平  [n_samples,7,7,64] >> [n_samples, 7*7*64]  -1 表示不用管维度
##h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])
##h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
##h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)  # 之前需要定义keep_prob
##
###fun2 layer
##W_fc1 = weight_variable([1024,10])  # 1024 是输出
##b_fc1 = bias_variable([10])
##prediction = tf.nn.softmax()
### 2d -> 1d  扁平  [n_samples,7,7,64] >> [n_samples, 7*7*64]  -1 表示不用管维度
##h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])
##h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
##h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)  # 之前需要定义keep_prob  防止过拟合
